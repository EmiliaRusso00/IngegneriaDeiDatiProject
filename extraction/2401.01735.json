{
    "id_table_1": {
        "caption": "Table 1:Average payoffs (↑↑\\uparrow) of LLMs in the “senior environment”, where we hand-picked 5 LLMs to test the impact of variation in opponent types.",
        "table": [
            [],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.2.2\">\n          0.000\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.2.3\">\n          0.000\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.3.1.4.2.4\">\n          0.030\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.2.5\">\n          1.000\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.2.6\">\n          1.190\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.2.7\">\n          1.062\n         </td>\n        "
            ],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.3.2\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.5.3.2.1\">\n           0.450\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.3.3\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.5.3.3.1\">\n           0.392\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.3.1.5.3.4\">\n          0.370\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.3.5\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.5.3.5.1\">\n           1.454\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.3.6\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.5.3.6.1\">\n           1.200\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.3.7\">\n          0.991\n         </td>\n        "
            ],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.4.2\">\n          0.400\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.4.3\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.6.4.3.1\">\n           0.392\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.3.1.6.4.4\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.6.4.4.1\">\n           0.410\n          </span>\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.4.5\">\n          1.012\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.4.6\">\n          0.870\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.4.7\">\n          <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.6.4.7.1\">\n           1.206\n          </span>\n         </td>\n        "
            ],
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.7.5.2\">\n          0.100\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.7.5.3\">\n          0.183\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T1.3.1.7.5.4\">\n          0.170\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.7.5.5\">\n          0.734\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.7.5.6\">\n          1.063\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.7.5.7\">\n          1.084\n         </td>\n        "
            ]
        ],
        "footnotes": [],
        "references": [
            "For the beauty contest games,as shown in Table                  1                ,                  Claude2                and                  GPT3.5                obtain the highest average payoffs among the selected models, with                  Claude2                does better than                  GPT3.5                for                          20                               20                              20                       sessions, and the reverse happens for                          100                               100                              100                       sessions.This implies that                  GPT3.5                is less variable in its behaviour than                  Claude2                and therefore has less volatile payoffs.While                  GPT4                displays the strongest rationality among the LLMs in the “rational environment”, it does not fare as well as                  GPT3.5                , which obtains the highest average payoffs in both the “melee” and “senior environment”.Table                  1                also shows for second-price auctions,                  Claude-I                receives much higher payoffs than the rest for                          20                               20                              20                       sessions, but                  GPT3.5                performs the best for                          100                               100                              100                       sessions.                  GPT3.5                is among the LLMs that display higher rationality degree, and it also outperforms all the other LLMs in the “melee” and “senior environment”."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:Frequency in the format of percentages (%) of breaking rules (↓↓\\downarrow) by different LLMs during the experiments in our economics arena.\nThe results are obtained through tracking 150 runs without history and 180 runs with history of two kinds of game.",
        "table": [
            [],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.2\">\n          100.00\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.3\">\n          6.11\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.4\">\n          90.00\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.5\">\n          0.00\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.6\">\n          13.89\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.7\">\n          12.22\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.8\">\n          10.56\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.9\">\n          3.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.1.4.2.10\">\n          0.00\n         </td>\n        "
            ],
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.3\">\n          6.67\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.4\">\n          5.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.5\">\n          80.67\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.6\">\n          0.00\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.7\">\n          12.67\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.8\">\n          13.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.9\">\n          12.67\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.10\">\n          13.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.5.3.11\">\n          0.00\n         </td>\n        "
            ],
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.2\">\n          4.44\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.3\">\n          3.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.4\">\n          78.89\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.5\">\n          0.00\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.6\">\n          11.11\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.7\">\n          6.67\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.8\">\n          7.78\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.9\">\n          3.33\n         </td>\n         ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.3.1.6.4.10\">\n          0.00\n         </td>\n        "
            ]
        ],
        "footnotes": [],
        "references": [
            "During the above experiments, we also observed that certain LLMs cannot strictly follow the game instructions, i.e. fail to give responses in the specified format or break the game rules, which reflects the ability of LLMs to follow instructions in natural languages.So, we track the frequency of rule-breaking of each agent in each type of games, and the results are given in Table                  2                .By comparing the beauty contests and the auctions, we can see that the frequency of breaking rules is higher in the later.The rule break frequency is also higher for set-ups without history than with.Considering that prompts in auctions are more complex than the ones in beauty contests, as well as the fact that adding history makes the prompts even more complicated, we argue that certain types of LLMs fail to follow the instructions because they cannot understand the prompts with higher complexity.Therefore, we argue that our economics arena can also reflect the ability of LLMs to strictly follow the natural language instructions."
        ]
    }
}