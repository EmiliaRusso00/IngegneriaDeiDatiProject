{
    "id_table_1": {
        "caption": "Table 1:Performance Comparison Between Methods With and Without Chain of Summarization: Interaction frequency is approximately 55 times in 10 seconds, assuming each API call takes about 1.66 seconds and game . Using the Chain of Summarization significantly accelerates decision-making speed and reduces the number of API calls.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.1.3.2.1\">\n        Without Chain of Summarization\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.1.3.2.2\">\n        7,000\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.1.3.2.3\">\n        70\n       </td>\n      "
            ]
        ],
        "footnotes": [],
        "references": [
            "We chose GPT3.5-turbo-16k as our Large Language Model (LLM) and conducted tests in scenarios both with and without the Chain of Summarization (CoS) method. Notably, the python-sc2 package facilitates an interaction frequency of around 55 times every 10 seconds. Table                  1                presents the comparative results of using CoS and not using it.",
            "Here, we evaluated only GPT3.5-Turbo, GPT3.5-Turbo-16k, GPT4, and models finetuned with data collected via GPT, namely Finetune-ChatGlm2 6b and Finetune-Llama2 7b. When testing the capabilities of different Large Language Models (LLMs), we uniformly applied the chain of summarization method and consistent prompts to assess their performance in the TextStarCraft II environment(Figure                  1                ). In this context, we set LLMs to function as Protoss players, contending against built-in Zerg opponents of Harder."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:Comparison of LLM Agents’ Win Rates Against Various Difficulty Levels of Built-in AI in TextStarCraft II with Chain of Summarization Using Different Prompts. Notably, Prompt2 significantly enhances LLM agent performance, enabling victories against the Harder difficulty level of built-in AI.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.1\">\n        GPT3.5-turbo-16k with prompt1\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.2\">\n        2/2\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.3\">\n        TBD\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.4\">\n        TBD\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.5\">\n        0/2\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.6\">\n        TBD\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.3.2.7\">\n        TBD\n       </td>\n      "
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.1\">\n        GPT3.5-turbo-16k with prompt2\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.2\">\n        8/8\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.3\">\n        9/9\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.4\">\n        8/8\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.5\">\n        21/25\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.6\">\n        7/14\n       </td>\n       ",
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.7\">\n        0/12\n       </td>\n      "
            ]
        ],
        "footnotes": [],
        "references": [
            "We used the GPT3.5-turbo-16k model as our LLM, applying the Chain of Summarization method with two different types of prompts. In this context, ’prompt’ specifically refers to the prompts used in the multi-frame summarization method. We evaluated the effectiveness of these prompts in the TextStarCraft II environment, where the LLMs, playing as Protoss, faced off against Zerg opponents of varying difficulties. The results, displayed in Table                  2                , demonstrate that Prompt2 significantly enhances the performance of the LLM agents."
        ]
    }
}